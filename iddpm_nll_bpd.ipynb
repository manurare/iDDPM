{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a href=\"https://colab.research.google.com/github/sndnyang/iDDPM/blob/master/iddpm_nll_bpd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# iDDPM Evaluation NLL BPD\n",
    "\n",
    "Negative Log Likelihood, namely Bits Per Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.7 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Download the pre-trained model from openAI iDDPM\n",
    "!wget https://openaipublic.blob.core.windows.net/diffusion/march-2021/cifar10_uncond_50M_500K.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboardX torch_fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If you are use colab, you can mount your Google Drive.\n",
    "# Otherwise, you need to upload the code?\n",
    "import sys\n",
    "\n",
    "sys.path.append('/drive/MyDrive/research/iDDPM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from ExpUtils import *\n",
    "from utils.dataloader import datainfo, dataload\n",
    "from eval_tasks import *\n",
    "from iddpm.script_util import create_model_and_diffusion, model_and_diffusion_defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.7 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "arg = argparse.Namespace()\n",
    "arg.dataset = 'cifar10'\n",
    "arg.data_path = './data'\n",
    "arg.workers = 4\n",
    "arg.eval = 'nll'\n",
    "arg.resume = 'cifar10_uncond_50M_500K.pt'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "arg.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_info = datainfo(logger, arg)\n",
    "\n",
    "if arg.eval == 'buffer':\n",
    "    buffer = torch.load(arg.resume, map_location=arg.device)\n",
    "    eval_buffer(buffer, arg)\n",
    "\n",
    "normalize = [transforms.Normalize(mean=data_info['stat'][0], std=data_info['stat'][1])]\n",
    "augmentations = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    *normalize\n",
    "])\n",
    "\n",
    "'''\n",
    "    model\n",
    "'''\n",
    "\n",
    "model, diffusion = create_model_and_diffusion(\n",
    "    **model_and_diffusion_defaults()\n",
    ")\n",
    "\n",
    "\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Number of params: {format(n_parameters, \",\")}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint = torch.load(arg.resume, map_location=arg.device)\n",
    "\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.load_state_dict(checkpoint)\n",
    "model = model.to(arg.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_bpd_on_dataset(nll_model, f, loader, arg):\n",
    "    all_bpd = []\n",
    "    c = 0\n",
    "    start = time.time()\n",
    "    for images, _ in loader:\n",
    "        images = images.to(arg.device)\n",
    "        c += images.shape[0]\n",
    "        minibatch_metrics = nll_model.calc_bpd_loop(f, images, clip_denoised=True)\n",
    "        total_bpd = minibatch_metrics[\"total_bpd\"]\n",
    "        total_bpd = total_bpd.mean()\n",
    "        all_bpd.append(total_bpd.item())\n",
    "        if c % 100 == 0:\n",
    "            print(f'{c} bpd: {total_bpd.item()}')\n",
    "    bpd = np.mean(all_bpd)\n",
    "    end = time.time()\n",
    "    print(f\"done {c} samples: bpd={bpd}, takes {end - start}\")\n",
    "    return bpd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://github.com/openai/guided-diffusion/blob/main/guided_diffusion/gaussian_diffusion.py#L709\n",
    "from iddpm.gaussian_diffusion import GaussianDiffusion as NllDiffusion, LossType, ModelVarType, ModelMeanType\n",
    "\n",
    "nll_model = NllDiffusion(\n",
    "    betas=diffusion.betas,\n",
    "    model_mean_type=ModelMeanType.EPSILON,\n",
    "    model_var_type=ModelVarType.LEARNED,\n",
    "    loss_type=LossType.MSE,\n",
    "    rescale_timesteps=False,\n",
    ")\n",
    "\n",
    "train_set, val_dataset = dataload(arg, augmentations, normalize, data_info)\n",
    "\n",
    "# make sure that batch size doesn't matter, since the model use laynorm, not batch norm. small batch size is very slow.\n",
    "train_loader = torch.utils.data.DataLoader(val_dataset, batch_size=100, shuffle=False, pin_memory=True, num_workers=arg.workers)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=100, shuffle=False, pin_memory=True, num_workers=arg.workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_bpd = run_bpd_on_dataset(nll_model, model, val_loader, arg)\n",
    "\n",
    "print(f'test bpd {val_bpd}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_bpd = run_bpd_on_dataset(nll_model, model, train_loader, arg)\n",
    "print(f'train bpd {train_bpd}, test bpd {val_bpd}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
